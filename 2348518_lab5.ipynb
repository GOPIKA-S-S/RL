{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyNSWIKAfi/C2rAudTFteBXl",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/GOPIKA-S-S/RL/blob/main/2348518_lab5.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "txYUCkzUvMX6",
        "outputId": "7d9f584f-2624-4a7b-bd72-e3eadd539824"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (1.26.4)\n"
          ]
        }
      ],
      "source": [
        "!pip install numpy\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import random\n",
        "\n",
        "# Set up a simple grid world\n",
        "class GridWorld:\n",
        "    def __init__(self, width, height, start, goal):\n",
        "        self.width = width\n",
        "        self.height = height\n",
        "        self.start = start\n",
        "        self.goal = goal\n",
        "        self.agent_pos = start\n",
        "\n",
        "    def reset(self):\n",
        "        self.agent_pos = self.start\n",
        "        return self.agent_pos\n",
        "\n",
        "    def step(self, action):\n",
        "        if action == 0:  # Up\n",
        "            self.agent_pos = (max(self.agent_pos[0] - 1, 0), self.agent_pos[1])\n",
        "        elif action == 1:  # Down\n",
        "            self.agent_pos = (min(self.agent_pos[0] + 1, self.height - 1), self.agent_pos[1])\n",
        "        elif action == 2:  # Left\n",
        "            self.agent_pos = (self.agent_pos[0], max(self.agent_pos[1] - 1, 0))\n",
        "        elif action == 3:  # Right\n",
        "            self.agent_pos = (self.agent_pos[0], min(self.agent_pos[1] + 1, self.width - 1))\n",
        "\n",
        "        reward = 1 if self.agent_pos == self.goal else 0\n",
        "        return self.agent_pos, reward\n",
        "\n",
        "\n",
        "class MonteCarloControl:\n",
        "    def __init__(self, env, n_episodes=10000, discount_factor=0.9, epsilon=0.1):\n",
        "        self.env = env\n",
        "        self.n_episodes = n_episodes\n",
        "        self.discount_factor = discount_factor\n",
        "        self.epsilon = epsilon\n",
        "        self.q_values = np.zeros((env.height, env.width, 4))  # Q-values for each state-action pair\n",
        "        self.policy = np.zeros((env.height, env.width), dtype=int)  # Best action for each state\n",
        "        self.policy_stable = False\n",
        "\n",
        "    def get_action(self, state):\n",
        "        if random.random() < self.epsilon:\n",
        "            return random.randint(0, 3)  # Explore: random action\n",
        "        return self.policy[state]  # Exploit: best action according to policy\n",
        "\n",
        "    def generate_episode(self):\n",
        "        state = self.env.reset()\n",
        "        episode = []\n",
        "        done = False\n",
        "\n",
        "        while not done:\n",
        "            action = self.get_action(state)\n",
        "            new_state, reward = self.env.step(action)\n",
        "            episode.append((state, action, reward))\n",
        "            if new_state == self.env.goal:\n",
        "                done = True\n",
        "            state = new_state\n",
        "\n",
        "        return episode\n",
        "\n",
        "    def update_q_values(self, episode):\n",
        "        G = 0\n",
        "        visited = np.zeros((self.env.height, self.env.width, 4))  # To keep track of visited state-action pairs\n",
        "\n",
        "        for state, action, reward in reversed(episode):\n",
        "            G = self.discount_factor * G + reward\n",
        "            if visited[state[0], state[1], action] == 0:\n",
        "                self.q_values[state[0], state[1], action] += G\n",
        "                visited[state[0], state[1], action] += 1\n",
        "\n",
        "        # Update policy\n",
        "        for s in range(self.env.height):\n",
        "            for t in range(self.env.width):\n",
        "                best_action = np.argmax(self.q_values[s, t])\n",
        "                self.policy[s, t] = best_action\n",
        "\n",
        "    def learn(self):\n",
        "        for episode in range(self.n_episodes):\n",
        "            episode_data = self.generate_episode()\n",
        "            self.update_q_values(episode_data)\n",
        "\n",
        "            if (episode + 1) % 1000 == 0:\n",
        "                print(f\"Episode {episode + 1}: Policy updated.\")\n",
        "\n",
        "# Main Execution\n",
        "if __name__ == \"__main__\":\n",
        "    env = GridWorld(width=4, height=4, start=(0, 0), goal=(3, 3))\n",
        "    mc_control = MonteCarloControl(env)\n",
        "    mc_control.learn()\n",
        "\n",
        "    # Display the learned policy\n",
        "    print(\"Learned Policy (0=Up, 1=Down, 2=Left, 3=Right):\")\n",
        "    print(mc_control.policy)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_243rS0Twejv",
        "outputId": "1a121e2b-141a-4af5-802b-098f4094d1f1"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Episode 1000: Policy updated.\n",
            "Episode 2000: Policy updated.\n",
            "Episode 3000: Policy updated.\n",
            "Episode 4000: Policy updated.\n",
            "Episode 5000: Policy updated.\n",
            "Episode 6000: Policy updated.\n",
            "Episode 7000: Policy updated.\n",
            "Episode 8000: Policy updated.\n",
            "Episode 9000: Policy updated.\n",
            "Episode 10000: Policy updated.\n",
            "Learned Policy (0=Up, 1=Down, 2=Left, 3=Right):\n",
            "[[3 3 3 1]\n",
            " [0 0 3 1]\n",
            " [0 0 0 1]\n",
            " [0 0 0 0]]\n"
          ]
        }
      ]
    }
  ]
}